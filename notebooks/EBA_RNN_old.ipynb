{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "# Recurrent Neural Network\n",
    "\n",
    "Let's try throwing a neural network at the demand forecasting problem.  We'll give input method the day of the week, time of day, day of the year, and temperature.  This first version uses a single recurrent cell, with a linear layer at the end.  This could be enhanced by making deeper networks at both the beginning and end, using a fancier cell (LSTM, GRU).\n",
    "\n",
    "THis is the old messy version (which at least worked!)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from util.get_weather_data import convert_isd_to_df, convert_state_isd\n",
    "from util.EBA_util import remove_na, avg_extremes\n",
    "\n",
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.contrib.layers import fully_connected\n",
    "from tensorflow.contrib.rnn import BasicRNNCell,LSTMCell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "#Extend to multiple temperature series\n",
    "try:\n",
    "    df_joint=pd.read_csv('data/pdx_joint.txt',\n",
    "        index_col=0, parse_dates=True)\n",
    "    print('Read in PDX Frame from file')\n",
    "except:\n",
    "    print('Creating PDX DataFrame from scratch')\n",
    "    air_df = pd.read_csv('data/air_code_df.gz')\n",
    "    #Just get the weather station data for cities in Oregon.\n",
    "    df_weather=convert_state_isd(air_df,'OR')\n",
    "    #Select temperature for only Portland\n",
    "    #msk1=np.array(df_weather['city']=='Portland')\n",
    "    #select temp for all Oregon stations\n",
    "    msk2=np.array(df_weather['state']=='OR')\n",
    "    df_pdx_weath=df_weather.loc[msk2]\n",
    "    #find number of unique station city/state combinations\n",
    "    Nstation = len(df_pdx_weath['city, state'].unique())\n",
    "\n",
    "    #reshape the single temperature column into Nstation copies.  \n",
    "    unique_station=df_pdx_weath['city, state'].unique()\n",
    "    temp_df=pd.DataFrame()\n",
    "    for station in unique_station:\n",
    "        colname=str('Temp-'+station)\n",
    "        temp_df[colname]=df_pdx_weath.loc[df_pdx_weath['city, state']==station,'Temp']\n",
    "\n",
    "    #get electricity data for Portland General Electric\n",
    "    df_eba=pd.read_csv('data/EBA_time.gz',index_col=0,parse_dates=True)\n",
    "    msk=df_eba.columns.str.contains('Portland')\n",
    "    df_pdx=df_eba.loc[:,msk]\n",
    "    #select out demand data\n",
    "    msk1 = df_pdx.columns.str.contains('[Dd]emand') \n",
    "    dem=df_pdx.loc[:,msk1]\n",
    "    #Make a combined Portland Dataframe for demand vs weather.\n",
    "    df_joint=pd.DataFrame(dem)\n",
    "    df_joint=df_joint.join(temp_df)\n",
    "    df_joint = df_joint.rename(columns={df_joint.columns[0]:'Demand',\n",
    "             df_joint.columns[1]:'Forecast'})\n",
    "    df_joint.to_csv('data/pdx_joint.txt')\n",
    "\n",
    "#Make copies of data from dataframe to avoid overwriting source data.\n",
    "dem=df_joint['Demand'].copy()\n",
    "temp=df_joint.loc[:,df_joint.columns.str.contains('Temp')].copy()\n",
    "fore=df_joint['Forecast'].copy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "#clean up data, remove NA\n",
    "#remove NA values, and average extreme values down\n",
    "for y in [temp,dem]:\n",
    "    if len(y.shape)>1:\n",
    "        for i in range(y.shape[1]):\n",
    "            x= y.iloc[:,i]\n",
    "            x = remove_na(x)\n",
    "            y.iloc[:,i] = avg_extremes(x)\n",
    "    else:\n",
    "        x= y\n",
    "        x = remove_na(x)\n",
    "        y = avg_extremes(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "def make_temptime_data(temp_mat):\n",
    "    \"\"\"make_input_data\n",
    "    Takes input temperature data matrix (for multiple locations),\n",
    "    and extends with extra indices for time of day, day of year, day of week, and holiday. \n",
    "\n",
    "    Input: temp_mat - pandas series of temperatures a location.  \n",
    "    Output: in_mat - matrix of raw temperatures, and scaled times of day and year.\n",
    "    \"\"\"\n",
    "    Tind = temp_mat.index\n",
    "    Nt=len(Tind)\n",
    "    hr = Tind.hour.values/(24-1)\n",
    "    #scale day of year to [0,1]\n",
    "    dyear = Tind.dayofyear.values/(365-1+Tind.is_leap_year.astype(int))\n",
    "    dweek = Tind.dayofweek.values/(7-1)\n",
    "    # #scale temperature data to so that max/min correspond to [0,1]  \n",
    "    # temp_max = temp_mat.max(axis=0)\n",
    "    # temp_min = temp_mat.min(axis=0)\n",
    "    # temp_mat = (temp_mat-temp_min)/(temp_max-temp_min)\n",
    "    out_mat=np.stack([hr,dweek,dyear]).T\n",
    "    out_mat= np.hstack([temp_mat.values,out_mat])\n",
    "    return out_mat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "# ##OBSOLETE - REMOVING!\n",
    "\n",
    "# def scale_demand(dem):\n",
    "#     \"\"\"scale_demand\n",
    "#     Scale demand to be on 0,1 scale.\n",
    "#     Input: demand - series at single location\n",
    "#     Output: dem_scale - scaled array of values.\n",
    "#             dem_max, dem_min - the maximum and minimum values.\n",
    "#     \"\"\"\n",
    "#     dem_scale = dem.values\n",
    "#     dem_max = np.max(dem_scale)\n",
    "#     dem_min = np.min(dem_scale)\n",
    "#     dem_scale = (dem_scale-dem_min)/(dem_max-dem_min)\n",
    "#     return dem_scale, dem_max,dem_min\n",
    "\n",
    "# #drop data prior to \n",
    "# temp_mat,tmax,tmin=make_temptime_data(temp[:Ntest])\n",
    "# dem_mat,dmax,dmin=scale_demand(dem)\n",
    "\n",
    "# temp_train = temp_mat[0:Ntest,:]\n",
    "# temp_test = temp_mat[Ntest:,:]\n",
    "# dem_train = dem_mat[0:Ntest]\n",
    "# dem_test = dem_mat[Ntest:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "#make combined temperature and time data.\n",
    "temp_mat=make_temptime_data(temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "#Use Sklearn MinMaxScaler to scale all data between 0,1.\n",
    "#Only fit the scaling on training data.\n",
    "\n",
    "Nt=len(dem)\n",
    "Ntest = Nt//2\n",
    "\n",
    "Tscaler=MinMaxScaler()\n",
    "Dscaler=MinMaxScaler()\n",
    "Tscaler.fit(temp_mat[:Ntest])\n",
    "Dscaler.fit(dem[:Ntest].values.reshape((Ntest,1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "def get_random_batch(X,y,n_batch,seq_len):\n",
    "    \"\"\"get_random_batch(Xsig,t,n_batch)   \n",
    "    Gets multiple random samples for the data.\n",
    "    Samples generated by 'get_selection' function.\n",
    "    Makes list of returned entries.\n",
    "    Then combines together with 'stack' function at the end.\n",
    "\n",
    "    X - matrix of inputs, (Nt, Ninputs)\n",
    "    y - vector of desired outputs (Nt)\n",
    "    n_batch - number of batches\n",
    "    seq_len - length of sequence to extract in each batch\n",
    "\n",
    "    Outputs:\n",
    "    X_batch - random subset of inputs shape (Nbatch,seq_len,Ninputs) \n",
    "    y_batch - corresponding subset of outputs (Nbatch,seq_len)\n",
    "    \"\"\"\n",
    "    Nt,Nin = X.shape\n",
    "    x_list=[]\n",
    "    y_list=[]\n",
    "    for i in range(n_batch):\n",
    "        n0=int(np.random.random()*(Nt-seq_len-1))\n",
    "        x_sub = X[n0:n0+seq_len]\n",
    "        y_sub = y[n0:n0+seq_len]\n",
    "        x_list.append(x_sub)\n",
    "        y_list.append(y_sub)\n",
    "    x_batch=np.stack(x_list,axis=0)\n",
    "    y_batch=np.stack(y_list,axis=0)\n",
    "    y_batch=y_batch.reshape( [n_batch,seq_len,-1])                    \n",
    "    return x_batch,y_batch\n",
    "\n",
    "Xb,yb=get_random_batch(temp_mat,dem_mat,1000,24)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "n_steps=24\n",
    "n_inputs=len(temp.iloc[0])+3\n",
    "n_neurons=120\n",
    "n_layers=3\n",
    "n_outputs=1  #number of stations to predict at that time.\n",
    "lr=1E-2\n",
    "np.random.seed(seed=3453)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "def make_RNN_cell(n_neurons,fn=tf.nn.relu):\n",
    "    cell=BasicRNNCell(num_units=n_neurons,activation=fn)\n",
    "    return cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "#Initial test with code liberally borrowed from ch14 of Geron's \n",
    "#\"Practical Machine Learning with scikit-learn and Tensorflow\"\n",
    "\n",
    "#Makes a single RNN cell, with a fully connected output layer (with no activation on the output).\n",
    "\n",
    "print('setting up graphs:Multi-layer RNN')\n",
    "tf.reset_default_graph()\n",
    "#inputs:  Nobs, with n_steps, and n_inputs per step\n",
    "X = tf.placeholder(tf.float32,[None,n_steps,n_inputs],name='X')\n",
    "#Outputs: n_outputs we want to predict in the future.\n",
    "y = tf.placeholder(tf.float32,[None,n_steps,n_outputs],name='y')\n",
    "\n",
    "#define neural network shape\n",
    "#works:make a list of them.  \n",
    "# cell=BasicRNNCell(num_units=n_neurons,activation=tf.nn.relu)\n",
    "\n",
    "#Make a list of cells to pass along.  \n",
    "cell_list=[]\n",
    "for i in range(n_layers):\n",
    "    cell_list.append(make_RNN_cell(n_neurons,tf.nn.relu))\n",
    "\n",
    "multi_cell=tf.contrib.rnn.MultiRNNCell(cell_list,state_is_tuple=True)\n",
    "#Note that using [cell]*n_layers did not work since that copies the memory location, rather than making\n",
    "#a number of independent copies.\n",
    "rnn_outputs,states=tf.nn.dynamic_rnn(multi_cell,X,dtype=tf.float32)\n",
    "#this maps the number of hidden units to fewer outputs.\n",
    "stacked_rnn_outputs = tf.reshape(rnn_outputs,[-1,n_neurons])\n",
    "stacked_outputs = fully_connected(stacked_rnn_outputs,n_outputs,activation_fn=None)\n",
    "outputs=tf.reshape(stacked_outputs,[-1,n_steps,n_outputs])\n",
    "\n",
    "#define loss (mean-square-error)\n",
    "loss = tf.reduce_mean(tf.square(outputs-y))\n",
    "#define optimization function.\n",
    "optimizer=tf.train.AdamOptimizer(learning_rate=lr)\n",
    "training_op=optimizer.minimize(loss)\n",
    "init=tf.global_variables_initializer()\n",
    "\n",
    "saver = tf.train.Saver()\n",
    "#Try adding everything by name to a collection to save and restore later\n",
    "tf.add_to_collection('X',X)\n",
    "tf.add_to_collection('y',y)\n",
    "tf.add_to_collection('loss',loss)\n",
    "tf.add_to_collection('pred',outputs)\n",
    "tf.add_to_collection('train',training_op)\n",
    "\n",
    "#compute number correct.\n",
    "print('Loading data')\n",
    "n_iter=1000\n",
    "n_batch=100\n",
    "run_network=True\n",
    "\n",
    "if (run_network==True):\n",
    "    print('Running this thang')\n",
    "    with tf.Session() as sess:\n",
    "        init.run()\n",
    "        for iteration in range(n_iter):\n",
    "            #select random starting point. \n",
    "            X_batch,y_batch=get_random_batch(\n",
    "                            temp_train, dem_train, n_batch, n_steps)\n",
    "\n",
    "            sess.run(training_op, feed_dict={X: X_batch, y:y_batch})\n",
    "            if iteration%50 ==0:\n",
    "                mse =loss.eval(feed_dict={X:X_batch,y:y_batch})\n",
    "                print(\"MSE on batch \",iteration,':\\t',mse)\n",
    "                #save model\n",
    "                saver.save(sess, \"./models/pdx_RNN_model\",\n",
    "                           write_meta_graph=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "So multiple tanhs are bad.  A couple ReLU layers seem to work well, but do lead to negative predictions.  Note that in comparisons that the early 2015 data is pretty flaky (like the forecasts are zero, and I had to fix multiple issues in the demand data)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "def model_predict_whole(Xin,path_str=\"pdx_RNN_model\"):\n",
    "    \"\"\"model_predict_whole(tstart)\n",
    "    Retrieve the outputs of the network for all values of the inputs \n",
    "    \"\"\"\n",
    "    Nt,Nin=Xin.shape\n",
    "    nmax = int(Nt/n_steps)\n",
    "    ytot = np.zeros((Nt,1))\n",
    "    #Note that loading/saving graph is not properly implemented yet.    \n",
    "    #reset graph, and reload saved graph\n",
    "    tf.reset_default_graph()\n",
    "    model_path = \"./models/\"+path_str    \n",
    "    saver = tf.train.import_meta_graph(model_path+\".meta\")\n",
    "    #saver=tf.train.import_meta_graph(full_model_name+'.meta')\n",
    "    #restore graph structure\n",
    "    X=tf.get_collection('X')[0]\n",
    "    y=tf.get_collection('y')[0]\n",
    "    outputs=tf.get_collection('pred')[0]\n",
    "    train_op=tf.get_collection('train_op')[0]\n",
    "    loss=tf.get_collection('loss')[0]\n",
    "    #restores weights etc.\n",
    "    #saver.restore(sess,full_model_name)\n",
    "    \n",
    "    with tf.Session() as sess:\n",
    "\n",
    "        #restore variables\n",
    "        saver.restore(sess,model_path)\n",
    "        for i in range(nmax-1):\n",
    "            n0=n_steps*i\n",
    "            x_sub = Xin[n0:n0+n_steps,:]\n",
    "            x_sub = x_sub.reshape(-1,n_steps,Nin)\n",
    "            y_pred=sess.run(outputs,feed_dict={X:x_sub})\n",
    "            #nn_pred=predict_on_batch(sess,X_batch)            \n",
    "            ytot[n0:n0+n_steps]=y_pred\n",
    "    return ytot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "def plot_whole_sample_fit(X,y,ntest,n_steps,path_str=\"pdx_RNN_model\"):\n",
    "    \"\"\"plot_whole_sample_fit\n",
    "\n",
    "    Plot ALL of the predictions of the trained model\n",
    "    on a 'test' set with different noise, and longer\n",
    "    times.  Concatenates the predicted results together.  \n",
    "    \"\"\"\n",
    "    #pull in the inputs, and predictions\n",
    "    Nt, Nin = X.shape\n",
    "    ytot=model_predict_whole(X,path_str)\n",
    "    plt.figure()\n",
    "    #now plot against the test sets defined earlier\n",
    "    plt.plot(np.arange(0,ntest),X[:ntest,0],'b',label='Training')\n",
    "    plt.plot(np.arange(ntest,Nt), X[ntest:,0],'g',label='Test')\n",
    "    plt.plot(np.arange(Nt),ytot,'r',label='Predicted')\n",
    "    plt.plot(np.arange(Nt),dem_mat,label='Real')\n",
    "    plt.legend(loc='right')\n",
    "    plt.show()\n",
    "    return ytot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "#n0,x_sub,y_pred=toy_predict(2.5)\n",
    "ytot=plot_whole_sample_fit(temp_mat,dem_mat,Ntest,n_steps,'pdx_RNN_model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "#convert the RNN output to a pandas time-series\n",
    "pred=pd.Series(((dmax-dmin)*ytot+dmin).reshape(-1),index=dem.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "def rmse(x,y):\n",
    "    z = np.sqrt(np.sum((x-y)*(x-y))/len(x))\n",
    "    return z\n",
    "\n",
    "def mape(x,y):\n",
    "    z = np.mean(np.abs((1-x/y)))\n",
    "    return z\n",
    "\n",
    "plt.plot(dem['2015-11':],pred['2015-11':],'.')\n",
    "plt.xlabel('Actual Demand')\n",
    "plt.ylabel('RNN Prediction')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "nt = len(ytot)//2\n",
    "fore_train_rmse=rmse(fore[:nt],dem[:nt])\n",
    "fore_test_rmse=rmse(fore[nt:],dem[nt:])\n",
    "pred_train_rmse=rmse(pred[:nt],dem[:nt])\n",
    "pred_test_rmse=rmse(pred[nt:],dem[nt:])\n",
    "\n",
    "print(\"Forecast RMSE in training/test      : {}, {}\".format(fore_train_rmse,fore_test_rmse))\n",
    "print(\"RNN Prediction RMSE in training/test: {}, {}\".format(pred_train_rmse,pred_test_rmse))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "fore_train_mape=mape(fore[:nt],dem[:nt])\n",
    "fore_test_mape=mape(fore[nt:],dem[nt:])\n",
    "\n",
    "pers_train_mape=mape(dem[:nt-24].values,dem[24:nt].values)\n",
    "pers_test_mape=mape(dem[nt:-24].values,dem[nt+24:].values)\n",
    "\n",
    "pred_train_mape=mape(pred[:nt],dem[:nt])\n",
    "pred_test_mape=mape(pred[nt:],dem[nt:])\n",
    "\n",
    "print(\"Forecast MAPE in training/test      : {}, {}\".format(fore_train_mape,fore_test_mape))\n",
    "print(\"Persistence MAPE in training/test   : {}, {}\".format(pers_train_mape,pers_test_mape))\n",
    "print(\"RNN Prediction MAPE in training/test: {}, {}\".format(pred_train_mape,pred_test_mape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "So this simple RNN does worse than the actual forecast, but does out perform persistence.  Well, that's at least something.\n",
    "Obviously, this can be greatly improved.  The above is a simple toy model, one input station, one output series for the same set of time.\n",
    "We can play with other architectures, activations, and using more data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,6))\n",
    "date_slice=slice('2016-12-20','2017-01-02')\n",
    "plt.plot(pred[date_slice],label='pred')\n",
    "plt.plot(dem[date_slice],label='demand')\n",
    "plt.plot(fore[date_slice],label='fore')\n",
    "plt.legend(loc='right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,6))\n",
    "date_slice=slice('2017-06-01','2017-08-01')\n",
    "plt.plot(pred[date_slice]/dem[date_slice]-1,label='pred err')\n",
    "plt.plot(fore[date_slice]/dem[date_slice]-1,label='fore err')\n",
    "plt.ylabel('Percentage Error')\n",
    "plt.legend(loc='right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "So looking at the percentage errors, this model (which currently lacks knowledge of holidays) is messing up on Thanksgiving.  Also the model seems to make opposite errors to the forecast model.  It's probably worth checking that the distribution of errors.  Eyeballing the curves shows that the errors are lowest early in the morning, and highest at midday.  The error signal probably has a significant daily frequency component.\n",
    "\n",
    "Right now this is a 3-layer RNN.  We can extend it to include different cell types, fiddle with the network size, and maybe a different layout.\n",
    "I'm going to retry this in a more modular approach (and for a more general set of code), with multiple inputs, differing sizes, dropout, more efficient loading."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "name": "EBA_RNN_old.ipynb"
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
